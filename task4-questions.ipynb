{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "What is the effect of increasing the number of layers in an MLP? Are larger (with a greater number of hidden layers) models preferred to smaller ones? If so, why? Or why not?\n",
    "\n",
    "   Your answer: **[The effect of increasing the number of layers in an MLP increases the complexity of the model. Thus, it increases the capacity of the model to capture more features. A larger network is not always preferred over a smaller one since the efficiency of the model depends on many factors. Such as - the depth of the network, problem, and data. A larger network might overfit, thus no generalization. In case of a lack of sufficient data, a larger network might not able to capture all features.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "What is the significance of activation functions in deep learning models? Name any two activation functions that you would use for a hidden layer and for the output layer. \n",
    "\n",
    "   Your answer: **[The activation function is added to deep learning models to add non-linearity and to learn complex patterns from the data. Ex - The linear functions can not solve the XOR problem. Since the problem is not linearly separable thus, a non-linearity is required. Hence, the activation function in the model helps to solve these complex problems. The two activation functions used for a hidden layer and the output layer are ReLu and Softmax functions.\n",
    "]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Assume you have a problem to predict the annual rainfall in a certain region with some historic numeric data that was given to you, which of these 2 models (Linear Regression and Logistic Regression) would you use? \n",
    "How would you modify the problem statement to use the other model? Are they both linear? \n",
    "\n",
    "   Your answer: **[The problem of predicting the annual rainfall in a certain region with some historic numeric data is a regression problem. Thus, I will choose Linear Regression to predict the annual rainfall value. To use Logistic Regression, the problem must be converted to binary classification. Thus, the modified problem predicts whether the annual rainfall is above or below and equal to 40 inches. Logistic Regression is not linear since the sigmoid function is used to compute the hypothesis.]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "What is the difference between a loss function and an optimization function? \n",
    "\n",
    "   Your answer: **[The optimization function is the function we want to either minimize or maximize. Ex- The problem of optimizing hours of productivity. In this case, the optimal value is the maximum value. The loss function is a type of optimization function whose optimal solution is the minimum value. It measures the difference between the model's predicted value, and the actual output value. Common choices are Mean square error, absolute error, and any distance metric defined over the space of target values that can act as a loss function. ]**\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "What will happen if you choose a very small or a very large learning rate? \n",
    "\n",
    "   Your answer: **[If we choose a very small learning rate, the gradient descent will take a long time to reach the optimal solution. And when the loss function is not convex, it might stuck at a local minimum or a saddle point. \n",
    "If we choose a very large learning rate, we might never reach the optimal solution due to the zig-zag effect where our gradient is bouncing back and forth by skipping the optimal solution.]**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "What is the function of perplexity in tSNE?  \n",
    "    \n",
    "   Your answer: **[The perplexity in tSNE is a tuneable parameter. It sets the effective number of neighbors that each point is attracted to or, in other words the parameter guesses the number of close neighbors for each point. Thus, for higher perplexity, clusters tend to shrink into denser structures.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
